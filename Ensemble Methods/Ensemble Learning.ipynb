{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "wr_ic4BokweV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "from sklearn.dummy import DummyRegressor\n",
        "from sklearn.metrics import mean_squared_error as mse\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
        "from sklearn.linear_model import LinearRegression, Lasso, Ridge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "xds-ZS1-k6R3"
      },
      "outputs": [],
      "source": [
        "dat = np.load('/content/drive/MyDrive/data.npz')\n",
        "X_train,y_train = dat['X_train'],dat['y_train']\n",
        "X_val,y_val = dat['X_val'],dat['y_val']\n",
        "X_test,y_test = dat['X_test'],dat['y_test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gimHGkHRmGqk",
        "outputId": "42c4b040-e073-48f8-ba05-973ec89611e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test MSE: 13704.069\n"
          ]
        }
      ],
      "source": [
        "def Dummy_Regr():\n",
        "  dummy_regr = DummyRegressor()\n",
        "  dummy_regr.fit(X_train, y_train)\n",
        "  y_pred = dummy_regr.predict(X_test)\n",
        "  print(f'Test MSE: {mse(y_test, y_pred):.3f}')\n",
        "\n",
        "Dummy_Regr()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6pWrxM0G3rI"
      },
      "source": [
        "Explain your idea and why it would be a reasonable baseline. Implement\n",
        "your baseline regressor and report the testing MSE.\n",
        "\n",
        "Ans: This is a dummy Regressor which always predicts the mean value of the training data. So it can be treated as a resonable baseline as the predictions demonstrate almost to no learning at all. The above value is the baseline test_MSE of dummy regressor which has not learnt any weights post training. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5OLzBst8Kune"
      },
      "outputs": [],
      "source": [
        "models = [\"OLS\", \"Ridge\", \"LASSO\"]\n",
        "def grid_search(X_train, X_val, X_test):\n",
        "\n",
        "    for model_name in models: \n",
        "\n",
        "      if model_name == \"OLS\":\n",
        "        best_alpha = None\n",
        "        model = LinearRegression()\n",
        "        model.fit(X_train, y_train)\n",
        "        y_val_hat = model.predict(X_val)\n",
        "        best_val_mse = mse(y_val, y_val_hat)\n",
        "        best_model = model\n",
        "        y_hat = model.predict(X_test)\n",
        "      else:\n",
        "        best_val_mse = float('inf')\n",
        "        best_model = None\n",
        "        best_alpha = None\n",
        "        for alpha in np.linspace(-10, 10, 20):\n",
        "          if model_name == 'LASSO':\n",
        "            model = Lasso(alpha=alpha)\n",
        "          else: \n",
        "            model = Ridge(alpha=alpha)\n",
        "          model.fit(X_train, y_train)\n",
        "          y_hat_val = model.predict(X_val)\n",
        "          val_mse = mse(y_val,y_hat_val)\n",
        "          if val_mse < best_val_mse:\n",
        "            best_val_mse = val_mse\n",
        "            best_model = model\n",
        "            best_alpha = alpha\n",
        "        y_hat = best_model.predict(X_test)\n",
        "\n",
        "      print(\"\")\n",
        "      print(f'Model Coeffs.: {best_model.coef_}')\n",
        "      print(f\"Model       : {model_name}\")\n",
        "      print(f\"Val_MSE     : {best_val_mse}\")\n",
        "      print(f\"Test_MSE    : {mse(y_test, y_hat)}\")\n",
        "      print(f\"Best lambda : {best_alpha}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbxGEUuVvPx2",
        "outputId": "baf375c3-7a67-4256-a1f2-2900546907c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Model Coeffs.: [ 6.78392782e-08 -9.46961087e-07 -2.01559722e-05 -9.44137992e-03\n",
            " -3.58051575e-01  7.99676039e-03  7.76618779e-01  3.36026253e-01\n",
            "  1.07191588e-02 -1.02676180e-01 -5.07021725e-03  1.70750137e-01\n",
            "  1.70548307e-01 -7.85617873e-02  8.08841109e-02  1.91114765e-02\n",
            "  1.51462054e-01  3.19292162e-02 -2.12960339e-01  2.88362100e-01\n",
            " -1.12241911e-02 -8.67993656e-01 -3.59423010e-01  6.18997673e-02\n",
            "  4.31522108e-03  2.16059287e-03  1.92880833e-02 -4.09630461e-02\n",
            "  6.54875549e-02 -2.05499951e-02  1.41233615e-01  5.99102193e-02\n",
            " -2.19882293e-04  8.13233954e-02 -1.98919313e-01  2.16878679e-04\n",
            "  2.25805759e-03  2.22044605e-16  4.05009131e-01 -6.47656867e-01\n",
            " -3.10081883e-01 -3.54953823e-01  6.57339759e-01  6.40429550e-01\n",
            "  3.85662550e-01 -3.70739286e-01 -6.63693444e-01  1.10864850e+00\n",
            " -5.97729892e-02  8.91788515e-01 -4.55125561e-01 -2.63133332e-01\n",
            " -5.58711691e-01]\n",
            "Model       : OLS\n",
            "Val_MSE     : 638.5917288630449\n",
            "Test_MSE    : 11377.149595301527\n",
            "Best lambda : None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_ridge.py:148: LinAlgWarning: Ill-conditioned matrix (rcond=4.25919e-19): result may not be accurate.\n",
            "  overwrite_a=True).T\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_ridge.py:148: LinAlgWarning: Ill-conditioned matrix (rcond=1.2772e-18): result may not be accurate.\n",
            "  overwrite_a=True).T\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_ridge.py:148: LinAlgWarning: Ill-conditioned matrix (rcond=2.12775e-18): result may not be accurate.\n",
            "  overwrite_a=True).T\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_ridge.py:148: LinAlgWarning: Ill-conditioned matrix (rcond=2.97756e-18): result may not be accurate.\n",
            "  overwrite_a=True).T\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_ridge.py:148: LinAlgWarning: Ill-conditioned matrix (rcond=3.82665e-18): result may not be accurate.\n",
            "  overwrite_a=True).T\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_ridge.py:148: LinAlgWarning: Ill-conditioned matrix (rcond=4.675e-18): result may not be accurate.\n",
            "  overwrite_a=True).T\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_ridge.py:148: LinAlgWarning: Ill-conditioned matrix (rcond=5.52264e-18): result may not be accurate.\n",
            "  overwrite_a=True).T\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_ridge.py:148: LinAlgWarning: Ill-conditioned matrix (rcond=6.36955e-18): result may not be accurate.\n",
            "  overwrite_a=True).T\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_ridge.py:148: LinAlgWarning: Ill-conditioned matrix (rcond=7.21574e-18): result may not be accurate.\n",
            "  overwrite_a=True).T\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_ridge.py:148: LinAlgWarning: Ill-conditioned matrix (rcond=8.06121e-18): result may not be accurate.\n",
            "  overwrite_a=True).T\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Model Coeffs.: [ 6.78336415e-08 -9.46622047e-07 -2.01546319e-05 -9.43948746e-03\n",
            " -3.57667644e-01  7.98769849e-03  7.76191279e-01  3.36071899e-01\n",
            "  1.09373463e-02 -1.02666138e-01 -5.07456109e-03  1.70729499e-01\n",
            "  1.70550256e-01 -7.85276767e-02  8.08215003e-02  1.91097430e-02\n",
            "  1.51455006e-01  3.19769274e-02 -2.12941821e-01  2.87967800e-01\n",
            " -1.12149728e-02 -8.67555137e-01 -3.59462193e-01  6.16768682e-02\n",
            "  4.31445667e-03  2.16439237e-03  1.92744938e-02 -4.09447337e-02\n",
            "  6.54656118e-02 -2.05426505e-02  1.41233909e-01  5.99100855e-02\n",
            " -2.27811659e-04  8.13238195e-02 -1.98920470e-01  2.16896122e-04\n",
            "  2.25800812e-03  0.00000000e+00  4.05030540e-01 -6.42787538e-01\n",
            " -3.06672763e-01 -3.53512869e-01  6.53799183e-01  6.35338126e-01\n",
            "  3.82889292e-01 -3.69053430e-01 -6.61309331e-01  1.10361470e+00\n",
            " -6.29122480e-02  8.87735836e-01 -4.53244353e-01 -2.59531651e-01\n",
            " -5.54352953e-01]\n",
            "Model       : Ridge\n",
            "Val_MSE     : 638.5901256333506\n",
            "Test_MSE    : 11377.0929798394\n",
            "Best lambda : 10.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 13147767.588590682, tolerance: 3814.615671150009\n",
            "  positive)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 12942569.50163991, tolerance: 3814.615671150009\n",
            "  positive)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 24437.428039569408, tolerance: 3814.615671150009\n",
            "  positive)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 16046.613446239382, tolerance: 3814.615671150009\n",
            "  positive)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Model Coeffs.: [ 5.82117421e-08 -2.29799128e-05 -1.35085655e-05 -0.00000000e+00\n",
            " -0.00000000e+00  1.87157208e-02  0.00000000e+00  3.25733542e-02\n",
            "  4.61552767e-02 -0.00000000e+00 -8.49798340e-04  0.00000000e+00\n",
            "  5.59733578e-02  0.00000000e+00 -0.00000000e+00  4.10013618e-04\n",
            "  0.00000000e+00  1.02566189e-01  0.00000000e+00 -0.00000000e+00\n",
            " -2.14813365e-02  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            " -0.00000000e+00 -0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
            "  0.00000000e+00 -0.00000000e+00  1.97680773e-01  0.00000000e+00\n",
            " -1.87258135e-02  2.54629575e-02 -1.79679921e-01  4.39740825e-04\n",
            "  1.96134130e-03  0.00000000e+00  0.00000000e+00 -0.00000000e+00\n",
            "  0.00000000e+00 -0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "  0.00000000e+00 -0.00000000e+00 -0.00000000e+00  0.00000000e+00\n",
            " -0.00000000e+00  0.00000000e+00 -0.00000000e+00 -0.00000000e+00\n",
            " -0.00000000e+00]\n",
            "Model       : LASSO\n",
            "Val_MSE     : 634.5342810078747\n",
            "Test_MSE    : 11339.621227295804\n",
            "Best lambda : 10.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4631.367847021669, tolerance: 3814.615671150009\n",
            "  positive)\n"
          ]
        }
      ],
      "source": [
        "grid_search(X_train, X_val, X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRKAqTGGBYUm",
        "outputId": "1bddfb74-8b17-4ddb-824b-1530e247870f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Model Coeffs.: [ 4.52849309e-01 -2.23140363e-02 -2.26974778e+00 -1.84115062e-01\n",
            " -8.38522184e+00  4.00769020e+00  6.87992051e+01  2.40985306e+01\n",
            "  8.77405547e-01 -1.23485928e+00 -1.87308527e+00 -3.00590459e+11\n",
            "  3.97802187e+00 -4.33924858e+00  1.84647889e-01  6.28758829e+00\n",
            "  2.46546761e+11  4.22101606e-01 -1.09002660e+01  6.08389717e+00\n",
            " -5.32820872e+00 -7.18790912e+01 -2.43987728e+01  4.77850407e+00\n",
            "  1.25723867e+00  8.00248383e-01  1.51475343e+11 -6.93806682e-01\n",
            "  4.93493804e+00 -2.80440697e+00 -2.38865349e+11  2.22882104e+11\n",
            " -1.67104729e-02  2.91197922e+11 -4.15406799e+00  7.12585449e-02\n",
            "  2.47277069e+00  5.83006817e+09  7.92939186e-01  4.90484477e+11\n",
            "  5.26718161e+11  5.32952948e+11  5.47715823e+11  5.23769030e+11\n",
            "  5.30072548e+11  5.17256389e+11 -3.97547272e+12 -3.81927501e+12\n",
            " -3.90428999e+12 -4.05512450e+12 -4.03245710e+12 -3.96244649e+12\n",
            " -3.98550008e+12]\n",
            "Model       : OLS\n",
            "Val_MSE     : 638.5407275830877\n",
            "Test_MSE    : 11377.604447368545\n",
            "Best lambda : None\n",
            "\n",
            "Model Coeffs.: [ 4.51213406e-01 -4.17680709e-03 -2.29803734e+00 -2.03919285e-01\n",
            " -1.13197850e+01  6.77399908e+00  8.82563085e+01  2.14619883e+01\n",
            " -8.93054041e+00 -1.21602867e+00 -1.33725707e+00  5.38485109e+00\n",
            "  3.97586258e+00 -4.97852844e+00  1.99451014e-01  6.47418350e+00\n",
            "  5.83476139e+00  2.16076050e-01 -1.09530828e+01  8.68946142e+00\n",
            " -7.98633896e+00 -9.04965533e+01 -2.21600100e+01  1.40697725e+01\n",
            "  1.42629662e+00  3.11376514e-01  1.18892840e+00 -7.78352783e-01\n",
            "  5.53715360e+00 -3.42458018e+00  1.14660899e+01  3.57801735e+00\n",
            "  6.09985726e-01  6.66685910e+00 -4.16297335e+00  7.29312154e-02\n",
            "  2.48661996e+00  4.65235265e-12  7.85034583e-01 -2.14617408e-01\n",
            " -1.14425127e-01 -1.34281609e-01  2.32830131e-01  2.11896275e-01\n",
            "  1.27487139e-01 -1.33366757e-01 -2.29046697e-01  3.73014629e-01\n",
            " -1.75853303e-02  3.19137182e-01 -1.57024114e-01 -8.85712479e-02\n",
            " -1.89538950e-01]\n",
            "Model       : Ridge\n",
            "Val_MSE     : 638.3735995147847\n",
            "Test_MSE    : 11369.233442522504\n",
            "Best lambda : -0.526315789473685\n",
            "\n",
            "Model Coeffs.: [-0.         -0.         -0.         -0.         -0.          0.\n",
            "  0.          0.          1.54528309 -0.18337436 -0.          0.\n",
            "  2.26898351  0.         -0.          0.          0.          3.22301008\n",
            "  0.         -0.          0.          0.          0.          0.\n",
            " -0.         -0.          0.         -0.          0.         -0.\n",
            " 12.74524383 -0.         -0.          3.35616572 -4.05063704  0.\n",
            "  1.04698793  0.          0.37877953 -0.          0.         -0.\n",
            "  0.          0.          0.         -0.         -0.          0.\n",
            " -0.          0.         -0.          0.         -0.        ]\n",
            "Model       : LASSO\n",
            "Val_MSE     : 638.4730375389579\n",
            "Test_MSE    : 11306.217091001508\n",
            "Best lambda : 0.5263157894736832\n"
          ]
        }
      ],
      "source": [
        "scaler = preprocessing.StandardScaler().fit(X_train)\n",
        "\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "grid_search(X_train_scaled, X_val_scaled, X_test_scaled)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_7W7hEgN4RR"
      },
      "source": [
        "i. Do you observe any difference on the learned coefficients among three\n",
        "models? Explain.\n",
        "\n",
        "As you can see, the regularization penalty actually depends on the magnitude of the coefficients, which in turn depends on the magnitude of the features themselves. So there you have it, when you change the scale of the features you also change the scale of the coefficients, which are thus penalized differently, resulting in different solutions. The exponents were mostly in negative range for most of the coefficients. They are adjusted and we can see coefficients with more positive exponential values and many exponents are closer to 0.\n",
        "\n",
        "ii. Compare test_MSE with those in (b). Which methods have obvious changes\n",
        "and which not? Explain\n",
        "\n",
        "test_MSE of OLS has not changed which means there is no effect of standardization but significantly impacts L1 and L2 penalized regression. There is no term affecting the coefficients for OLS (like $\\lambda$ for L1 and L2). So it is unaffected by change in scale of the values. \n",
        "\n",
        "With LASSO (L1), it fairs better than Ridge (L2) as it reduces the coefficients of insignificant features to 0 whereas L2 penalty is not strict and doesnt reduce the coefficients to 0 but to a lower value. So, this works well for feature selection in case we have a huge number of features. \n",
        "\n",
        "Due to reason stated above, as a result, we can see the test_MSE of L1 < test_MSE of L2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvE04eZbJ000",
        "outputId": "6ef06246-7ee0-438c-8225-403acba2c4b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max Depth.  : 3\n",
            "Val_MSE     : 505.0707734117222\n",
            "Test_MSE    : 10152.585792514701\n"
          ]
        }
      ],
      "source": [
        "def CART():\n",
        "  best_val_mse = float('inf')\n",
        "  best_model = None\n",
        "  best_height = 1\n",
        "  for i in range(1,11):\n",
        "    \n",
        "    CARTRegr = DecisionTreeRegressor(criterion='mse', max_depth=i, max_features=None, random_state=0)\n",
        "    CARTRegr.fit(X_train, y_train)\n",
        "    y_hat_val = CARTRegr.predict(X_val)\n",
        "    val_mse = mse(y_val, y_hat_val)\n",
        "    if val_mse < best_val_mse:\n",
        "        best_val_mse = val_mse\n",
        "        best_model = CARTRegr\n",
        "        best_height = i\n",
        "  \n",
        "  y_hat = best_model.predict(X_test)\n",
        "  print(f\"Max Depth.  : {best_height}\")\n",
        "  print(f\"Val_MSE     : {best_val_mse}\")\n",
        "  print(f\"Test_MSE    : {mse(y_test, y_hat)}\")\n",
        "  return best_model\n",
        "\n",
        "decision_tree_estimator = CART()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3IrNEjMXmDJ",
        "outputId": "7091729b-a569-4d62-fb0e-997b40b77b49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Estimator. : 28\n",
            "Val_MSE         : 411.19748505922365\n",
            "Test_MSE        : 9285.749130124314\n"
          ]
        }
      ],
      "source": [
        "def RandomForests():\n",
        "  best_val_mse = float('inf')\n",
        "  best_model = None\n",
        "  best_estimator = 1\n",
        "  for i in range(2,31):\n",
        "    RandForesRegr = RandomForestRegressor(n_estimators=i, criterion='mse', max_depth=3)\n",
        "    RandForesRegr.fit(X_train, y_train)\n",
        "    y_hat_val = RandForesRegr.predict(X_val)\n",
        "    val_mse = mse(y_val, y_hat_val)\n",
        "    if val_mse < best_val_mse:\n",
        "        best_val_mse = val_mse\n",
        "        best_model = RandForesRegr\n",
        "        best_estimator = i\n",
        "  \n",
        "  y_hat = best_model.predict(X_test)\n",
        "  print(f\"Best Estimator. : {best_estimator}\")\n",
        "  print(f\"Val_MSE         : {best_val_mse}\")\n",
        "  print(f\"Test_MSE        : {mse(y_test, y_hat)}\")\n",
        "RandomForests()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rq4HBJglrD62",
        "outputId": "8806e3f7-78c8-4eb4-9fa8-1182d81b66f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best lr.    : 1\n",
            "Val_MSE     : 540.6539292581144\n",
            "Test_MSE    : 8950.906714711358\n"
          ]
        }
      ],
      "source": [
        "def AdaBoost():\n",
        "  best_val_mse = float('inf')\n",
        "  best_model = None\n",
        "  best_lr = 1\n",
        "  for i in np.linspace(1e-1,2,20):\n",
        "    AdaBoostRegr = AdaBoostRegressor(base_estimator=decision_tree_estimator, learning_rate=i, n_estimators=28)\n",
        "    AdaBoostRegr.fit(X_train, y_train)\n",
        "    y_hat_val = AdaBoostRegr.predict(X_val)\n",
        "    val_mse = mse(y_val, y_hat_val)\n",
        "    if val_mse < best_val_mse:\n",
        "        best_val_mse = val_mse\n",
        "        best_model = AdaBoostRegr\n",
        "        best_estimator = i\n",
        "  \n",
        "  y_hat = best_model.predict(X_test)\n",
        "  print(f\"Best lr.    : {best_lr}\")\n",
        "  print(f\"Val_MSE     : {best_val_mse}\")\n",
        "  print(f\"Test_MSE    : {mse(y_test, y_hat)}\")\n",
        "AdaBoost()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFBrMF3m24QX"
      },
      "source": [
        "(g) Do those regressors learn from the data compared with the baseline? Compare and comment on (explain) their performances relative to the baseline, and relative to each other.\n",
        "\n",
        "| Model  | Val_MSE   |  test_MSE |  \n",
        "|---|---|---|\n",
        "| DummyRegressor  | N/A   |  13704.069 |\n",
        "| LinearRegressor  | 638.54  | 11377.60  |\n",
        "| Ridge(L2)  |638.37   |  11369.23 |\n",
        "| LASSO(L1)  | 638.47  | 11306.21  |\n",
        "|CART|505.07| 10152.58|\n",
        "| RandomForests  | 411.19   | 9285.74  | \n",
        "|  AdaBoost | 540.65  |  8950.90 | \n",
        "\n",
        "As we can see from the table, test_MSE is reducing as we use more capable regressors and show that they have learnt better than the baseline model which doesnt change in state/prediction. \n",
        "\n",
        "Linear regression is weaker than other algorithms in terms of reducing error rates. Its possibly due to not being able to represent the features linearly. In simple linear regression, outliers can significantly disrupt the outcomes.\n",
        "\n",
        "With LASSO (L1), it fairs better than Ridge (L2) as it reduces the coefficients of insignificant features to 0 whereas L2 penalty is not strict and doesnt reduce the coefficients to 0 but to a lower value. So, this works well for feature selection in case we have a huge number of features. As a result, we can see the test_MSE of L1 < test_MSE of L2\n",
        "\n",
        "The CART model gives high importance to a particular set of features. But the random forest chooses features randomly during the training process. Therefore, it does not depend highly on any specific set of features. Thus, the random forest can generalize over the data in a better way. This randomized feature selection makes random forest much more accurate than a decision tree.\n",
        "\n",
        "With random forests, you train however many decision trees using samples of BOTH the data points and the features. From there, each decision should be de-correlated. You can them take an average of the prediction (regression) \n",
        "\n",
        "With AdaBoost, you combine predictors by adaptively weighting the difficult-to-regress samples more heavily. From there, you make predictions based on the predictions of the various weak learners in the ensemble.This additive model (ensemble) works in a forward stage-wise manner, introducing a weak learner to improve the shortcomings of existing weak learners. \n",
        "\n",
        "If you carefully tune parameters, Adaboost results in better performance than random forests.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDbfiEUrG2X0"
      },
      "source": [
        "(h) Some of the features only have 0/1 values because they are converted from\n",
        "categorical features. Now for some of those categorical features, you modify the\n",
        "converted numerical values to 0/12345 (i.e., that attribute is either 0 or 12345) for all data including training, val and test and then do the regression problem again with the same random seed. Will your regressor give different estimations compared to your previous results? Give Yes/No answers for linear regression, CART, random forest and Adaboost, and explain why. Note: Assume that the change of value doesn’t affect any random process such as the\n",
        "feature sampling in random forest\n",
        "\n",
        "\n",
        "1.   Linear Regression : Yes. These values would be directly interpreted as a data point and would lead to different predictions and hence the categorical features have to be one hot encoded. \n",
        "2.   Random Forests, CART, AdaBoost are unaffected by some of the scaling methods. Trees are not affected by scaling because the splitting criterion first orders the values of each feature and then calculate the gini/entropy of the split. When CART looks for the best splits, it going to use entropy or gini to calculate information gain, this is not dependent on the scale of your predictor variable, rather on the resultant purity of the variable. Some scaling methods keep this order, so no change to the accuracy score as you should see the same variables and order of selection, just different cut off points.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "EE660_MT_Q3.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
